---
title: "Learn Generalised Additive (Mixed) Models"
author: "Stefano Coretta"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
theme_set(theme_minimal())
library(mgcv)
library(tidygam)
```

# Part I: Introduction

## Generalised additive models

* **G**enrealised **A**dditive **M**odel**s** (GAMs)

* $y = f(x)$
    * $f(x)$ = some function of $x$ (or *smooth function*)

## Smooth terms

* LMs have only **parametric terms**

    * `f0 ~ vowel + voicing + duration`
    
    * Parametric terms fit linear effects.

* GAMs add (non-parametric) **smooth terms** (or simply smooths, also smoothers):

    * `f0 ~ vowel + voicing + s(duration)`
    
    * `f(x)`: *some function of $x$*.
    
    * Smooth terms fit non-linear effects.

```r
library(mgcv)
gam(y ~ s(x), data)
```

The model: $y$ as *some* function of $x$





## Pupil size

* **Pupillometry data** from English young and older adults (McLaughlin et al 2022, <https://doi.org/10.3758/s13423-021-01991-0>).

* **Word recognition task** (verbal stimulus + verbal response).

* Words with **sparse and dense neighbourhood** density.

* **Hypotheses**:

  * Recognizing words with more competitors (dense neighbourhood) should come at a greater cognitive cost (greater pupil size) relative to recognizing words with fewer competitors (sparse neighbourhood).
  
  * The cognitive demands associated with increased neighbourhood density (greater pupil size) should be greater for older adults compared with young adults.

* The original study used Growth Curve Analysis (GCA).

* We will apply GAMs instead.

* CAVEAT: We are analysing the whole time course, rather than just a subset as done in the original study.



```{r pdq}
pdq_20 <- readRDS("data/pdq_20.rds") %>%
  mutate(
    Condition = factor(Condition, levels = c("Sparse", "Dense")),
    Age = factor(Age, levels = c("YA", "OA")),
    pupil_z = (pupil.binned - mean(pupil.binned)) / sd(pupil.binned)
  )

pdq_20
```



```{r pdq-traj}
ggplot(pdq_20, aes(timebins, pupil_z)) + geom_point(alpha = 0.01) + facet_grid(Condition ~ Age)
```






## A simple GAM



```{r pdq-gam, cache=TRUE}
pdq_gam <- bam(
  # Outcome
  pupil_z ~
    # Smooth over timebins
    s(timebins),
  data = pdq_20
)
```



```{r pdq-gam-sum}
summary(pdq_gam)
```



```{r pdq-gam-pred}
predict_gam(pdq_gam)
```



```{r pdq-gam-plot}
predict_gam(pdq_gam) %>% plot(series = "timebins")
```



```{r pdq-gam-plot-2}
predict_gam(pdq_gam, length_out = 100) %>% plot(series = "timebins")
```






## Number of knots `k`

* The "wiggliness" of the resulting spline is partially constrained by the number of *knots* (`k`).

* The more knots, the more wiggly the spline can be. Or the more knots the less smooth the spline can be.

* You can set the number of knots `k` with the argument `k` in `s()`.

<br>

```{r pdq-gam-2, cache=TRUE}
pdq_gam_2 <- bam(
  pupil_z ~
    s(timebins, k = 3),
  data = pdq_20
)
```



```{r pdq-gam-2-plot}
predict_gam(pdq_gam_2, length_out = 25) %>% plot(series = "timebins")
```



```{r pdq-gam-2-2, cache=TRUE}
pdq_gam_2 <- bam(
  pupil_z ~
    s(timebins, k = 20),
  data = pdq_20
)
```



```{r pdq-gam-2-2-plot}
predict_gam(pdq_gam_2, length_out = 100) %>% plot(series = "timebins")
```


## Comparing groups

* Comparing levels from a variable can be achieved with the **`by`-variable method**,

    * i.e. by specifying the variable as the value of the `by` argument in `s()`.

To use `by`-variables you need to:

* Change factor to an **ordered factor**.

* Change factor contrast to **treatment contrast** (`contr.treatment`).
    * The default in ordered factors is `contr.poly`, this won't work.
    
* Include the factor as a **parametric term**.

* Include a **reference smooth** and a **difference smooth** with the `by`-variable.

```{r pbq-ord}
pdq_20 <- pdq_20 %>%
  mutate(
    # Make the variables into an ordered factor
    Condition_o = as.ordered(Condition),
    Age_o = as.ordered(Age)
  )

# Change the contrasts to treatment
contrasts(pdq_20$Condition_o) <- "contr.treatment"
contrasts(pdq_20$Age_o) <- "contr.treatment"
```

Let's start with `Age_o`.

```{r pdq-gam-3, cache=TRUE}
pdq_gam_3 <- bam(
  pupil_z ~
    # Parametric term
    Age_o +
    # Reference smooth (Age_0 == "YA")
    s(timebins, k = 20) +
    # Difference smooth
    s(timebins, by = Age_o, k = 20),
  data = pdq_20
)
```



```{r pdq-gam-3-sum}
summary(pdq_gam_3)
```



```{r pdq-gam-3-plot}
predict_gam(pdq_gam_3, length_out = 100) %>% plot(series = "timebins", comparison = "Age_o")
```



```{r pdq-gam-3-diff}
pdq_gam_3_diff <- get_difference(
  pdq_gam_3, series = "timebins", length_out = 100,
  compare = list(Age_o = c("OA", "YA"))
)
pdq_gam_3_diff
```



```{r pdq-gam-3-diff-plot}
pdq_gam_3_diff %>% plot()
```


## Random effects



* Only **fixed effects** so far...

  * Parametric terms.
  * Smooth terms.



* **G**eneralised **A**dditive **M**ixed **M**odels (GAMMs).

* Two ways of including random effects:

  * Use the `"re"` basis function (`bs` argument in `s()`) for random intercept and slopes.

  * Include a **random smooth** term with the **factor smooth interaction** as basis (`bs = "fs"`).
  


* The **factor smooth interaction** method is more robust.



* **Factor smooth interaction**:
    * `bs = "fs"`.
    * A smooth is fitted at each level of a factor.
    * NOTE: it has *interaction* in the name but has nothing to do with interactions

* The random effect variable *needs to be a factor*.



Let's change `subject` to a factor (no need to make it an ordered factor).

```{r pdq-fac}
pdq_20 <- pdq_20 %>%
  mutate(
    subject = as.factor(subject)
  )
pdq_20
```




```{r pdq-gam-4, cache=TRUE}
pdq_gam_4 <- bam(
  pupil_z ~
    # Paramteric term
    Age_o +
    # Reference smooth
    s(timebins, k = 20) +
    # Difference smooth
    s(timebins, by = Age_o, k = 20) +
    # Factor smooth interaction by subject
    s(timebins, subject, bs = "fs", m = 1),
  data = pdq_20
)
```



```{r pdq-gam-4-sum}
summary(pdq_gam_4)
```



```{r pdq-gam-4-plot}
predict_gam(pdq_gam_4, length_out = 100, exclude_terms = "s(timebins,subject)") %>%
  plot(series = "timebins", comparison = "Age_o")
```



```{r pdq-gam-4-plot-2}
predict_gam(pdq_gam_4, length_out = 100, values = c(Age_o = "YA")) %>% # filter only YA subjects
  filter(subject %in% c(1:10)) %>% plot(series = "timebins")
```

## Comparing across groups (interactions)



* Technically, GAMs **don't allow interactions**.

  * They are ADDITIVE (interactions require multiplication).



* We can get interaction-like comparisons by creating **factor interactions** and using them as `by`-variables.



Let's create a factor interaction between `Age_o` and `Condition_o`.

We also need to make it into an ordered factor with treatment contrasts.

```{r pdq-int}
pdq_20 <- pdq_20 %>%
  mutate(
    Age_Cond = as.ordered(interaction(Age_o, Condition_o))
  )

contrasts(pdq_20$Age_Cond) <- "contr.treatment"
```



```{r pdq-gam-5, cache=TRUE}
pdq_gam_5 <- bam(
  pupil_z ~
    # Paramteric term
    Age_Cond +
    # Reference smooth
    s(timebins, k = 20) +
    # Difference smooth
    s(timebins, by = Age_Cond, k = 20) +
    # Factor smooth interaction by subject
    s(timebins, subject, bs = "fs", m = 1),
  data = pdq_20
)
```



```{r pdq-gam-5-sum}
summary(pdq_gam_5)
```




```{r pdq-gam-5-plot}
predict_gam(pdq_gam_5, length_out = 100, exclude_terms = "s(timebins,subject)") %>%
  plot(series = "timebins", comparison = "Age_Cond")
```



```{r pdq-gam-5-plot-2}
pdq_gam_5_pred_2 <- predict_gam(
  pdq_gam_5, length_out = 100, exclude_terms = "s(timebins,subject)",
  separate = list(Age_Cond = c("Age", "Condition"))
) %>%
  # The separate arguments returns variables with default alphabetical order.
  # Let's reorder the levels in Condition and Age.
  mutate(
    Condition = factor(Condition, levels = c("Sparse", "Dense")),
    Age = factor(Age, levels = c("YA", "OA")),
  )
```



```{r pdq-gam-5-pred-2-plot}
pdq_gam_5_pred_2 %>% plot(series = "timebins", comparison = "Condition") + facet_grid(~ Age)
```




```{r pdq-gam-5-diff}
pdq_gam_5_diff <- get_difference(
  pdq_gam_5, series = "timebins", length_out = 100, exclude_terms = "s(timebins,subject)",
  compare = list(Age_Cond = c("YA.Dense", "YA.Sparse"))
)
```


```{r pdq-gam-5-diff-plot}
pdq_gam_5_diff %>% plot()
```




# Part II: Hands-on

## The study

In the hands-on you will use the data from the paper by Bettelou Los et al. *The decline of local anchoring: A quantitative investigation* (2023), published in English Language and Linguistics (<https://www.doi.org/10.1017/S1360674323000047>).

The abstract of the paper should give you enough background to understand the context of the data. Here it is:

This article presents a quantitative study of the referential status of PPs in clause-initial position in the history of English. Earlier work (Los 2009; Dreschler 2015) proposed that main-clause-initial PPs in Old English primarily function as ‘local anchors’, linking a new clause to the immediately preceding discourse. As this function was an integral part of the verb-second (V2) constraint, the decline of local anchors was attributed to the loss of V2 in the fifteenth century, so that only the contrasting and frame-setting functions of these PPs remain in PDE. This article tests these hypotheses in the syntactically parsed corpora of OE, ME, EModE and LModE texts, using the Pentaset-categories (*New, Inert, Assumed, Inferred or Identity*; Komen 2011), based on Prince's categories (Prince 1981). The finding is that *Identity* clause-initial PPs decline steeply from early ME onwards, which means the decline pre-dates the loss of V2. A likely trigger is the loss of the OE paradigm of demonstrative, which functioned as standalone demonstrative pronouns as well as demonstrative determiners, and the loss of gender marking more generally. From EModE onwards, main-clause-initial PPs that still link to the preceding discourse do so much more indirectly, by an *Inferred* link.

### The data

To speed things up, here is the code to read and wrangle the `eng_hist.csv` file. (This workshop uses tidyverse code, it does not matter if you are not very comfortable with it, focus on the GAMs code instead and feel free to use base R.)

Note that the `eng_hist.csv` file does not contain counts of PPs, but rather occurences of PPs with information on English period, text ID and Pentaset category.

We have to do some data wrangling and then get the counts in `eng_count` below.

```{r eng-hist}
eng_hist <- read_csv("data/eng_hist.csv")

eng_filt <- eng_hist %>%
  # Filter out excluded items
  filter(Include == 1) %>%
  mutate(
    # Create a numeric version of EnglishPeriod for fitting GAMs.
    # Smooths only work with numeric variables.
    period = case_when(
      EnglishPeriod == "OE" ~ 1,
      EnglishPeriod == "ME" ~ 2,
      EnglishPeriod == "eModE" ~ 3,
      EnglishPeriod == "lModE" ~ 4
    ),
    EnglishPeriod = factor(EnglishPeriod, levels = c("OE", "ME", "eModE", "lModE")),
    Pentaset = factor(Pentaset, levels = c("Identity", "Inferred", "Assumed", "Inert", "New")),
    # Needed as factor for factor smooths (random effects)
    TextId = as.factor(TextId)
  )

# We obtain the counts of PPs by period, Pentaset category and text.
# We keep number of words in the text (Words) to input it as offset in the GAM later.
eng_count <- eng_filt %>%
  group_by(EnglishPeriod, period, TextId, Pentaset, Words) %>%
  count()
```

The following is a description of the variables in `eng_count`:

- `EnglishPeriod`: the historical period (OE, ME, eModE, ModE).
- `period`: the historical period as a number (1:4).
- `TextId`: the text ID.
- `Pentaset`: the Pentaset category of the occurrence.
- `Words`: the number of words in the text.
- `n`: the number of PPs in the specified text and Pentaset category.

Here is a plot showing the proportion of PP counts by Pentaset and English period. This is what we will model using GAMs.

```{r eng-hist-plot}
eng_filt %>%
  ggplot(aes(EnglishPeriod, fill = Pentaset)) +
  geom_bar(position = "fill") +
  scale_fill_brewer(palette = "Set1")
```

### Prep the data for a GAM

Now, let's prepare the data for fitting.

- You should convert the `Pentaset` column to an ordered factor (so we can use it as a `by`-variable in the GAMM).
- Change the contrasts of `Pentaset` to `contr.treatment`.
- Make sure `TextId` is a factor (so we can use it with factor smooth interactions, i.e. random smooths).

```{r eng-hist-prep}

```

Now we can fit the GAMM. We need:

- `n` as the outcome variable in the model.
- A parametric effect of `Pentaset`.
- A reference smooth over `period`. Which value for `k` shall we use?
- A reference smooth over `period` with `Pentaset` set as the `by`-variable. Which value for `k` shall we use?
- We will include classic random effects with `re` (in the paper we did this because there was no added benefit in using factor smooth interactions and using `re` speeded up computation). I added the code for you below.
- We will also add an "offset" to the model. Offsets are a robust way to normalise counts when using raw counts rather than proportions (i.e. `n` is the raw count of PPs out of the number of words in `Words`). I will add this bit for you in the code below.
- Since our outcome variable is a count, we need to use the Poisson family instead of the Gaussian (which is the default). (For an introduction to Poisson models, see <https://doi.org/10.1111/lnc3.12439> by Winter and Bürkner).

So far we've been using the `gam()` function. The alternative `bam()` is designed to be more computationally efficient when working with big amounts of data (`bam` for "Big gAM"). It never hurts to also use it with smaller amounts of data and there isn't a simple formula to tell what big or small amounts actually mean. So just go with `bam()`

```{r eng-hist-gam}
eng_hist_gam <- bam(
  ... ~
    # Fill in with all the needed terms/smooths
    ... +
    # Random effects
    s(TextId, bs = "re", m = 1) +
    # This is the code to include an offset.
    # We will get estimated counts per 100k words (because we are dividing `Words` by 100k).
    offset(log(Words/100000)),
  data = eng_count,
  # Fill in the right family.
  family = ...,
  # We can set discrete to TRUE to speed up computation.
  discrete = TRUE
)
```

If you are completely lost (or you want to check what you did), you can have a look at the code from the paper here: <https://github.com/stefanocoretta/2020-penta/blob/bd9e4d6c56bc9d2206f1de9d5e49cff0073ae168/2020-penta.qmd#L201>. **BUT GIVE IT A TRY YOURSELF FIRST!** :)

Now that you have fitted the model, why don't you try to interpret the summary?

Remember:

- The parametric terms tell you if there are differences in smooth HEIGHT relative to the reference (Intercept).

- The smooth terms tell you if there are differences in smooth SHAPE relative to the reference smooth.

```{r eng-hist-gam-sum}
summary(eng_hist_gam)
```


Go ahead and get the predictions for the model with `predict_gam()`.

- Make sure you exclude the random effect `re` term.
- Since we included an offset in the model, we need to specify a value for `Words` (by default, the average value from the range in the data is selected). You can do so with the `values` argument of `predict_gam()`, which takes a named vector of values (see `?predict_gam` for examples).

Familiarise yourself with the output.

```{r}

```

Now go ahead and plot the predictions.

```{r}

```

What can you say about the change in PP numbers by Pentaset category?
