---
title: "Learn Generalised Additive (Mixed) Models"
author: "Stefano Coretta"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
theme_set(theme_minimal())
library(mgcv)
library(tidygam)
```

[INSERT CODE FROM SLIDES]

## Hands-on

### The study

In the hands-on you will use the data from the paper by Bettelou Los et al. *The decline of local anchoring: A quantitative investigation* (2023), published in English Language and Linguistics (<https://www.doi.org/10.1017/S1360674323000047>).

The abstract of the paper should give you enough background to understand the context of the data. Here it is:

This article presents a quantitative study of the referential status of PPs in clause-initial position in the history of English. Earlier work (Los 2009; Dreschler 2015) proposed that main-clause-initial PPs in Old English primarily function as ‘local anchors’, linking a new clause to the immediately preceding discourse. As this function was an integral part of the verb-second (V2) constraint, the decline of local anchors was attributed to the loss of V2 in the fifteenth century, so that only the contrasting and frame-setting functions of these PPs remain in PDE. This article tests these hypotheses in the syntactically parsed corpora of OE, ME, EModE and LModE texts, using the Pentaset-categories (*New, Inert, Assumed, Inferred or Identity*; Komen 2011), based on Prince's categories (Prince 1981). The finding is that *Identity* clause-initial PPs decline steeply from early ME onwards, which means the decline pre-dates the loss of V2. A likely trigger is the loss of the OE paradigm of demonstrative, which functioned as standalone demonstrative pronouns as well as demonstrative determiners, and the loss of gender marking more generally. From EModE onwards, main-clause-initial PPs that still link to the preceding discourse do so much more indirectly, by an *Inferred* link.

### The data

To speed things up, here is the code to read and wrangle the `eng_hist.csv` file. (This workshop uses tidyverse code, it does not matter if you are not very comfortable with it, focus on the GAMs code instead and feel free to use base R.)

Note that the `eng_hist.csv` file does not contain counts of PPs, but rather occurences of PPs with information on English period, text ID and Pentaset category.

We have to do some data wrangling and then get the counts in `eng_count` below.

```{r eng-hist}
eng_hist <- read_csv("data/eng_hist.csv")

eng_filt <- eng_hist %>%
  # Filter out excluded items
  filter(Include == 1) %>%
  mutate(
    # Create a numeric version of EnglishPeriod for fitting GAMs.
    # Smooths only work with numeric variables.
    period = case_when(
      EnglishPeriod == "OE" ~ 1,
      EnglishPeriod == "ME" ~ 2,
      EnglishPeriod == "eModE" ~ 3,
      EnglishPeriod == "lModE" ~ 4
    ),
    EnglishPeriod = factor(EnglishPeriod, levels = c("OE", "ME", "eModE", "lModE")),
    Pentaset = factor(Pentaset, levels = c("Identity", "Inferred", "Assumed", "Inert", "New")),
    # Needed as factor for factor smooths (random effects)
    TextId = as.factor(TextId)
  )

# We obtain the counts of PPs by period, Pentaset category and text.
# We keep number of words in the text (Words) to input it as offset in the GAM later.
eng_count <- eng_filt %>%
  group_by(EnglishPeriod, period, TextId, Pentaset, Words) %>%
  count()
```

The following is a description of the variables in `eng_count`:

- `EnglishPeriod`: the historical period (OE, ME, eModE, ModE).
- `period`: the historical period as a number (1:4).
- `TextId`: the text ID.
- `Pentaset`: the Pentaset category of the occurrence.
- `Words`: the number of words in the text.
- `n`: the number of PPs in the specified text and Pentaset category.

Here is a plot showing the proportion of PP counts by Pentaset and English period. This is what we will model using GAMs.

```{r eng-hist-plot}
eng_filt %>%
  ggplot(aes(EnglishPeriod, fill = Pentaset)) +
  geom_bar(position = "fill") +
  scale_fill_brewer(palette = "Set1")
```

### Prep the data for a GAM

